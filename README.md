# Unified_Model
Unified Model小白入门学习记录

# 目录

## 生成部分

1. 生成模型分类
2. VAE变分自编码器
3. GAN

## 理解部分

## 统一模型

## 生成模型分类

生成模型是当前深度学习研究的重要方向之一，其目标是学习训练数据的潜在分布，并据此生成与原始数据相似的样本。大多数生成模型采用无监督学习的方式，通过大量未标注的数据来建模其内在结构和生成规律。根据建模方式的不同，生成模型可以大致分为两类：**显式生成模型（Explicit Generative Models）**与**隐式生成模型（Implicit Generative Models）**。

显式生成模型直接对数据的概率分布进行建模，即构造并显式定义一个可计算的概率密度函数。这一类方法又可以根据分布的可解性进一步细分：一类是分布**可精确求解**的模型，例如 PixelRNN 和 PixelCNN，它们通过自回归机制建模图像中每个像素点的条件分布，优点是生成过程具有强表达能力且分布形式清晰，但缺点是生成速度较慢且序列建模开销较大；另一类是分布**不可精确求解**但可通过变分推断近似的模型，如变分自编码器（VAE），该方法在可解释性和训练稳定性方面表现良好，但生成样本往往缺乏清晰度和多样性。

相比之下，隐式生成模型不直接建模数据的概率分布，而是通过某种方式**学习一个能够采样的过程**，使得生成的样本服从某种未知但近似于真实数据分布的分布。生成对抗网络（GAN）是其中的典型代表，它通过引入判别器与生成器之间的博弈机制，引导生成器逐步提高样本的逼真程度，具有生成质量高、细节丰富等优势，但训练过程不稳定、易出现模式崩溃（mode collapse）等问题。扩散模型（Diffusion Model）是近年来崛起的另一类隐式模型，通过模拟图像的逐步噪声添加与去噪过程来生成数据，虽然训练和采样成本较高，但在生成图像的质量、多样性和稳定性方面表现出色，逐渐成为最前沿的研究热点之一。

总体而言，显式模型在建模理论和可解释性上具有一定优势，但生成质量和效率方面存在瓶颈；隐式模型则更加灵活和高效，尤其在图像、语音等高维数据生成任务中展现出强大能力。实际应用中，往往需要在表达能力、训练难度与生成效率之间做出权衡。

# VAE变分自编码器

## 为什么使用后验而不是先验？

在生成模型中，我们希望通过边缘似然
$$
p(x) = \int p(x \mid z)\, p(z)\, dz
$$
来刻画复杂的数据分布 $p(x)$，其中先验 $p(z)$ 设为各维独立同分布的标准正态 $\mathcal{N}(0,I)$。

然而，直接从该先验随机采样 $z$ 并要求解码器生成图像 $x$ 面临两大难题：

1. 一方面，绝大多数 $z$ 并不对应任何真实样本，因而无法计算重构误差；
2. 另一方面，无法解析地求出上述积分，导致对 $p(x\mid z)$ 的极大似然训练不可行。

为此，我们转而对 **后验** $p(z\mid x)$ 做近似，利用可学习的对角高斯分布
$$
q_\phi(z \mid x) = \mathcal{N}(\mu_\phi(x), \operatorname{diag} \sigma_\phi^2(x))
$$
作为变分分布：编码器为每个输入 $x$ 输出 $\mu$ 与 $\sigma$，保证采样得到的每个 $z$ 都有对应的监督样本，从而避免“无效训练”。

结合重参数化技巧
$$
z = \mu + \sigma \odot \varepsilon,\quad \varepsilon \sim \mathcal{N}(0, I)
$$
和 ELBO:
$$
\mathcal{L} = \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] - D_{\mathrm{KL}}(q_\phi(z \mid x) \| p(z))
$$
我们即可同时学习解码器 $p_\theta(x\mid z)$（近似原本难以直接优化的 $p(x\mid z)$）和编码器 $q_\phi(z\mid x)$，在最大化下界的过程中既逼近真实后验又确保生成样本覆盖数据分布，实现对高斯混合式 $p(x)$ 的有效建模。

如果我们仅仅使用原始图像和生成图像的L2损失作为梯度，那么网络生成的图像将失去多样性，$q_\phi(z\mid x)$和$p(z)$ 将趋于同分布。

## 为什么latent空间要用标准正态分布而不用均匀分布

1. 标准正态分布具有良好的数学性质，具有良好的可导性、对称性、可分解性，这对于梯度优化、KL 散度计算等非常友好；

   - 标准正态分布易于使用重参数化技巧，使得梯度可回传：

   $$
   z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
   $$

   - 均匀分布没有简单可导采样公式，导致梯度传播难以实现或不稳定（尤其是在边界）。

2. 均匀分布在高维空间中分布在一个高维超立方体的边界上，采样值更可能集中在边界，而不是中心，导致解码器训练不稳定。

