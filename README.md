# Unified_Model
Unified Model小白入门学习记录

# 目录

## 生成部分

1. 生成模型分类
2. VAE变分自编码器
3. GAN

## 理解部分

## 统一模型

# VAE变分自编码器

## 为什么使用后验而不是先验？

在生成模型中，我们希望通过边缘似然
$$
p(x) = \int p(x \mid z)\, p(z)\, dz
$$
来刻画复杂的数据分布 $p(x)$，其中先验 $p(z)$ 设为各维独立同分布的标准正态 $\mathcal{N}(0,I)$。

然而，直接从该先验随机采样 $z$ 并要求解码器生成图像 $x$ 面临两大难题：

1. 一方面，绝大多数 $z$ 并不对应任何真实样本，因而无法计算重构误差；
2. 另一方面，无法解析地求出上述积分，导致对 $p(x\mid z)$ 的极大似然训练不可行。

为此，我们转而对 **后验** $p(z\mid x)$ 做近似，利用可学习的对角高斯分布
$$
q_\phi(z \mid x) = \mathcal{N}(\mu_\phi(x), \operatorname{diag} \sigma_\phi^2(x))
$$
作为变分分布：编码器为每个输入 $x$ 输出 $\mu$ 与 $\sigma$，保证采样得到的每个 $z$ 都有对应的监督样本，从而避免“无效训练”。

结合重参数化技巧
$$
z = \mu + \sigma \odot \varepsilon,\quad \varepsilon \sim \mathcal{N}(0, I)
$$
和 ELBO:
$$
\mathcal{L} = \mathbb{E}_{q_\phi(z \mid x)} [\log p_\theta(x \mid z)] - D_{\mathrm{KL}}(q_\phi(z \mid x) \| p(z))
$$
我们即可同时学习解码器 $p_\theta(x\mid z)$（近似原本难以直接优化的 $p(x\mid z)$）和编码器 $q_\phi(z\mid x)$，在最大化下界的过程中既逼近真实后验又确保生成样本覆盖数据分布，实现对高斯混合式 $p(x)$ 的有效建模。

如果我们仅仅使用原始图像和生成图像的L2损失作为梯度，那么网络生成的图像将失去多样性，$q_\phi(z\mid x)$和$p(z)$ 将趋于同分布。

## 为什么latent空间要用标准正态分布而不用均匀分布

1. 标准正态分布具有良好的数学性质，具有良好的可导性、对称性、可分解性，这对于梯度优化、KL 散度计算等非常友好；

   - 标准正态分布易于使用重参数化技巧，使得梯度可回传：

   $$
   z = \mu + \sigma \cdot \epsilon, \quad \epsilon \sim \mathcal{N}(0, I)
   $$

   - 均匀分布没有简单可导采样公式，导致梯度传播难以实现或不稳定（尤其是在边界）。

2. 均匀分布在高维空间中分布在一个高维超立方体的边界上，采样值更可能集中在边界，而不是中心，导致解码器训练不稳定。

